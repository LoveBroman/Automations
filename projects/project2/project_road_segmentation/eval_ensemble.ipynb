{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id",
        "outputId": "f017bf9a-3f9a-43f4-8390-fa9e3747cb62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation_models_pytorch\n",
            "  Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting efficientnet-pytorch==0.7.1 (from segmentation_models_pytorch)\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: huggingface-hub>=0.24.6 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.26.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (11.0.0)\n",
            "Collecting pretrainedmodels==0.7.4 (from segmentation_models_pytorch)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.17.0)\n",
            "Collecting timm==0.9.7 (from segmentation_models_pytorch)\n",
            "  Downloading timm-0.9.7-py3-none-any.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (4.66.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.5.1+cu121)\n",
            "Collecting munch (from pretrainedmodels==0.7.4->segmentation_models_pytorch)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation_models_pytorch) (6.0.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation_models_pytorch) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (4.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.26.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.0.2)\n",
            "Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=9782fa90e45ce53ed21120ffdd93988d88f0ec44d34fdb597a28c914a688f464\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=52625cc0c63fce1e96e5415ab2102684e9a98b566063eebf5a26597de299a7a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation_models_pytorch\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.12\n",
            "    Uninstalling timm-1.0.12:\n",
            "      Successfully uninstalled timm-1.0.12\n",
            "Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.3.4 timm-0.9.7\n",
            "Mounted at /content/gdrive\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.9 torchmetrics-1.6.0\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from functools import partial\n",
        "import matplotlib\n",
        "import matplotlib.image as mpimg\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "from google.colab import drive\n",
        "!pip install segmentation_models_pytorch\n",
        "drive.mount('/content/gdrive')\n",
        "import seaborn as sns\n",
        "import segmentation_models_pytorch as smp\n",
        "from segmentation_models_pytorch import Unet, UnetPlusPlus, DeepLabV3, PSPNet\n",
        "import segmentation_models_pytorch\n",
        "!pip install torchmetrics\n",
        "from torchmetrics.functional import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "###\n",
        "# Paths\n",
        "root_dir = \"data/training/\"\n",
        "image_dir = root_dir + \"images_extended_4_shadow_patches/\"\n",
        "gt_dir = root_dir + \"groundtruth_extended_4_shadow_patches/\"\n",
        "\n",
        "GRADIENT_COLORS = False\n",
        "if GRADIENT_COLORS:\n",
        "  test_image_dir = \"data/test_images_extended/\"\n",
        "else:\n",
        "  test_image_dir = \"data/test_set_images/\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "mXVMcU1-wSJ9"
      },
      "id": "mXVMcU1-wSJ9",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "###\n",
        "# Model related methods\n",
        "class PreloadedDataset(Dataset):\n",
        "    def __init__(self, images, ground_truth):\n",
        "        self.images = images\n",
        "        self.ground_truth = ground_truth\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        ground_truth = self.ground_truth[idx]\n",
        "        return image, ground_truth\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n",
        "        torch.nn.init.xavier_normal_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "def split(dataset, tr,val):\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [tr, val])\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "def get_dataloaders(train_dataset, val_dataset, batch_size):\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "###\n",
        "###\n",
        "# Image handling\n",
        "\n",
        "def load_npy(npy_file_path):\n",
        "  array = np.load(npy_file_path)\n",
        "  return array\n",
        "\n",
        "def load_image(infilename):\n",
        "    data = mpimg.imread(infilename)\n",
        "    return data\n",
        "\n",
        "def get_image_name(image_dir):\n",
        "    file_name = os.path.basename(image_dir)\n",
        "    return file_name\n",
        "\n",
        "def img_crop(im, w, h):\n",
        "    list_patches = []\n",
        "    imgwidth = im.shape[0]\n",
        "    imgheight = im.shape[1]\n",
        "\n",
        "    is_2d = len(im.shape) < 3\n",
        "    for i in range(0, imgheight, h):\n",
        "        for j in range(0, imgwidth, w):\n",
        "            if is_2d:\n",
        "                im_patch = im[j : j + w, i : i + h]\n",
        "            else:\n",
        "                im_patch = im[j : j + w, i : i + h, :]\n",
        "            list_patches.append(im_patch)\n",
        "    return list_patches\n",
        "\n",
        "def predict_patches(prediction, patch_threshold):\n",
        "    patches = img_crop(prediction, 16, 16)\n",
        "    iter = int(np.sqrt(len(patches)))\n",
        "\n",
        "    results = np.zeros((iter, iter))\n",
        "    c = 0\n",
        "    for j in range(iter):\n",
        "      for i in range(iter):\n",
        "        if patches[c].mean() > patch_threshold:\n",
        "          results[i, j] = 1\n",
        "        c = c + 1\n",
        "    return results\n",
        "\n",
        "def extract_number(folder_name):\n",
        "    return int(folder_name.split('_')[1])\n",
        "\n",
        "###\n",
        "###\n",
        "# Submission generation\n",
        "\n",
        "def masks_to_submission(submission_filename, results):\n",
        "    \"\"\"Converts images into a submission file\"\"\"\n",
        "    img_number = 0\n",
        "    with open(submission_filename, 'w') as f:\n",
        "        f.write('id,prediction\\n')\n",
        "        for res in results:\n",
        "            img_number = img_number + 1\n",
        "            f.writelines('{}\\n'.format(s) for s in mask_to_submission_strings(res, img_number))\n",
        "\n",
        "def mask_to_submission_strings(result, img_number):\n",
        "    \"\"\"Reads a single image and outputs the strings that should go into the submission file\"\"\"\n",
        "    im = result\n",
        "    patch_size = 16\n",
        "    for j in range(0, im.shape[1]):\n",
        "        for i in range(0, im.shape[0]):\n",
        "            label = result[i, j]\n",
        "\n",
        "            yield(\"{:03d}_{}_{},{}\".format(img_number, j*16, i*16, int(label)))\n",
        "\n",
        "def write_predictions_to_file(predictions, labels, filename):\n",
        "    max_labels = np.argmax(labels, 1)\n",
        "    max_predictions = np.argmax(predictions, 1)\n",
        "    file = open(filename, \"w\")\n",
        "    n = predictions.shape[0]\n",
        "    for i in range(0, n):\n",
        "        file.write(max_labels(i) + \" \" + max_predictions(i))\n",
        "    file.close()\n",
        "\n",
        "def remove_islands(prediction):\n",
        "  prediction = torch.tensor(prediction)\n",
        "  kernel = torch.ones((1, 1, 3, 3), dtype=prediction.dtype)\n",
        "  pred = torch.unsqueeze(prediction, dim=0)\n",
        "  neighbor_count = F.conv2d(pred, kernel, padding=1)\n",
        "  mask = (prediction == 1) & (neighbor_count == 1)\n",
        "  prediction = prediction.squeeze()\n",
        "  mask = mask.reshape((38,38))\n",
        "  prediction[mask] = 0\n",
        "  prediction = prediction.reshape((38,38))\n",
        "  return prediction\n",
        "\n",
        "def apply_crf(image, prob_map, sxy1=3, compat1=25, sxy2=50, srgb=30, compat2=10):\n",
        "\n",
        "  # CRF stuff starts\n",
        "  input_image = image.cpu().detach().numpy().copy() * 255\n",
        "  input_image = input_image.astype('uint8')\n",
        "  H,W,_ = input_image.shape\n",
        "\n",
        "  # Create the DenseCRF model\n",
        "  d = dcrf.DenseCRF2D(W, H, 2)  # W x H and 2 classes (road, non-road)\n",
        "\n",
        "  unary = unary_from_softmax(prob_map)\n",
        "  d.setUnaryEnergy(unary)\n",
        "  d.addPairwiseGaussian(sxy=3, compat=25)\n",
        "  d.addPairwiseBilateral(sxy=50, srgb=30, rgbim=input_image, compat=10)\n",
        "\n",
        "  # Perform CRF inference\n",
        "  # Produces a refined probability map with the same shape as input\n",
        "  max_iter = 10  # Number of CRF iterations\n",
        "  Q = d.inference(max_iter)\n",
        "\n",
        "  crf_output = np.array(Q).reshape((2, H, W))\n",
        "  crf_output = crf_output[1]  # Extract the \"road\" class probabilities\n",
        "\n",
        "  final_segmentation = (crf_output > 0.5).astype(np.uint8)  # Binary mask\n",
        "\n",
        "  return final_segmentation\n",
        "\n",
        "###\n",
        "###\n",
        "# Metrics\n",
        "\n",
        "def compute_f1(res, ans):\n",
        "    \"\"\"\n",
        "    Precision: tp/(tp+fp)\n",
        "    Recall: tp/(tp+fn)\n",
        "    F1 = 2 * (precision * recall) / (precision + recall)\n",
        "    \"\"\"\n",
        "\n",
        "    tp = torch.sum((res == 1) & (ans == 1))\n",
        "    fp = torch.sum((res == 1) & (ans != 1))\n",
        "    fn = torch.sum((res != 1) & (ans == 1))\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) != 0 else torch.tensor(0.0)\n",
        "    recall = tp / (tp + fn) if (tp + fn) != 0 else torch.tensor(0.0)\n",
        "    if precision + recall == 0:\n",
        "        return torch.tensor(0.0)\n",
        "    else:\n",
        "        return 2 * (precision * recall) / (precision + recall)\n"
      ],
      "metadata": {
        "id": "XimUSO0IwW8L"
      },
      "id": "XimUSO0IwW8L",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UnetPlusPlus(\n",
        "      encoder_name=\"resnet50\",        # Choose encoder\n",
        "      encoder_weights=\"imagenet\",    # Use pre-trained ImageNet weights\n",
        "      classes=1,                     # Number of output classes\n",
        "      activation=None               # No activation, as it's handled in loss/metrics\n",
        ")\n",
        "\n",
        "model2 = DeepLabV3(\n",
        "      encoder_name=\"resnet50\",        # Choose encoder\n",
        "      encoder_weights=\"imagenet\",    # Use pre-trained ImageNet weights\n",
        "      classes=1,                     # Number of output classes\n",
        "      activation=None               # No activation, as it's handled in loss/metrics\n",
        ")\n",
        "model.load_state_dict(torch.load(\"/content/gdrive/MyDrive/ML/predictions/U_NET++_1_base_case_100_im.pth\", map_location=torch.device('cpu')))\n",
        "model.load_state_dict(torch.load(\"/content/gdrive/MyDrive/ML/predictions/DEEPLAB_1_base_case_100_im.pth\", map_location=torch.device('cpu')))\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "mycbgdjkzMR6",
        "outputId": "88453846-04bf-4d56-cd53-70ffbe0d985d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "mycbgdjkzMR6",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 258MB/s]\n",
            "<ipython-input-4-a3c7589822a7>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/gdrive/MyDrive/ML/predictions/U_NET++_1_base_case_100_im.pth\", map_location=torch.device('cpu')))\n",
            "<ipython-input-4-a3c7589822a7>:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/gdrive/MyDrive/ML/predictions/DEEPLAB_1_base_case_100_im.pth\", map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for UnetPlusPlus:\n\tMissing key(s) in state_dict: \"decoder.blocks.x_0_0.conv1.0.weight\", \"decoder.blocks.x_0_0.conv1.1.weight\", \"decoder.blocks.x_0_0.conv1.1.bias\", \"decoder.blocks.x_0_0.conv1.1.running_mean\", \"decoder.blocks.x_0_0.conv1.1.running_var\", \"decoder.blocks.x_0_0.conv2.0.weight\", \"decoder.blocks.x_0_0.conv2.1.weight\", \"decoder.blocks.x_0_0.conv2.1.bias\", \"decoder.blocks.x_0_0.conv2.1.running_mean\", \"decoder.blocks.x_0_0.conv2.1.running_var\", \"decoder.blocks.x_0_1.conv1.0.weight\", \"decoder.blocks.x_0_1.conv1.1.weight\", \"decoder.blocks.x_0_1.conv1.1.bias\", \"decoder.blocks.x_0_1.conv1.1.running_mean\", \"decoder.blocks.x_0_1.conv1.1.running_var\", \"decoder.blocks.x_0_1.conv2.0.weight\", \"decoder.blocks.x_0_1.conv2.1.weight\", \"decoder.blocks.x_0_1.conv2.1.bias\", \"decoder.blocks.x_0_1.conv2.1.running_mean\", \"decoder.blocks.x_0_1.conv2.1.running_var\", \"decoder.blocks.x_1_1.conv1.0.weight\", \"decoder.blocks.x_1_1.conv1.1.weight\", \"decoder.blocks.x_1_1.conv1.1.bias\", \"decoder.blocks.x_1_1.conv1.1.running_mean\", \"decoder.blocks.x_1_1.conv1.1.running_var\", \"decoder.blocks.x_1_1.conv2.0.weight\", \"decoder.blocks.x_1_1.conv2.1.weight\", \"decoder.blocks.x_1_1.conv2.1.bias\", \"decoder.blocks.x_1_1.conv2.1.running_mean\", \"decoder.blocks.x_1_1.conv2.1.running_var\", \"decoder.blocks.x_0_2.conv1.0.weight\", \"decoder.blocks.x_0_2.conv1.1.weight\", \"decoder.blocks.x_0_2.conv1.1.bias\", \"decoder.blocks.x_0_2.conv1.1.running_mean\", \"decoder.blocks.x_0_2.conv1.1.running_var\", \"decoder.blocks.x_0_2.conv2.0.weight\", \"decoder.blocks.x_0_2.conv2.1.weight\", \"decoder.blocks.x_0_2.conv2.1.bias\", \"decoder.blocks.x_0_2.conv2.1.running_mean\", \"decoder.blocks.x_0_2.conv2.1.running_var\", \"decoder.blocks.x_1_2.conv1.0.weight\", \"decoder.blocks.x_1_2.conv1.1.weight\", \"decoder.blocks.x_1_2.conv1.1.bias\", \"decoder.blocks.x_1_2.conv1.1.running_mean\", \"decoder.blocks.x_1_2.conv1.1.running_var\", \"decoder.blocks.x_1_2.conv2.0.weight\", \"decoder.blocks.x_1_2.conv2.1.weight\", \"decoder.blocks.x_1_2.conv2.1.bias\", \"decoder.blocks.x_1_2.conv2.1.running_mean\", \"decoder.blocks.x_1_2.conv2.1.running_var\", \"decoder.blocks.x_2_2.conv1.0.weight\", \"decoder.blocks.x_2_2.conv1.1.weight\", \"decoder.blocks.x_2_2.conv1.1.bias\", \"decoder.blocks.x_2_2.conv1.1.running_mean\", \"decoder.blocks.x_2_2.conv1.1.running_var\", \"decoder.blocks.x_2_2.conv2.0.weight\", \"decoder.blocks.x_2_2.conv2.1.weight\", \"decoder.blocks.x_2_2.conv2.1.bias\", \"decoder.blocks.x_2_2.conv2.1.running_mean\", \"decoder.blocks.x_2_2.conv2.1.running_var\", \"decoder.blocks.x_0_3.conv1.0.weight\", \"decoder.blocks.x_0_3.conv1.1.weight\", \"decoder.blocks.x_0_3.conv1.1.bias\", \"decoder.blocks.x_0_3.conv1.1.running_mean\", \"decoder.blocks.x_0_3.conv1.1.running_var\", \"decoder.blocks.x_0_3.conv2.0.weight\", \"decoder.blocks.x_0_3.conv2.1.weight\", \"decoder.blocks.x_0_3.conv2.1.bias\", \"decoder.blocks.x_0_3.conv2.1.running_mean\", \"decoder.blocks.x_0_3.conv2.1.running_var\", \"decoder.blocks.x_1_3.conv1.0.weight\", \"decoder.blocks.x_1_3.conv1.1.weight\", \"decoder.blocks.x_1_3.conv1.1.bias\", \"decoder.blocks.x_1_3.conv1.1.running_mean\", \"decoder.blocks.x_1_3.conv1.1.running_var\", \"decoder.blocks.x_1_3.conv2.0.weight\", \"decoder.blocks.x_1_3.conv2.1.weight\", \"decoder.blocks.x_1_3.conv2.1.bias\", \"decoder.blocks.x_1_3.conv2.1.running_mean\", \"decoder.blocks.x_1_3.conv2.1.running_var\", \"decoder.blocks.x_2_3.conv1.0.weight\", \"decoder.blocks.x_2_3.conv1.1.weight\", \"decoder.blocks.x_2_3.conv1.1.bias\", \"decoder.blocks.x_2_3.conv1.1.running_mean\", \"decoder.blocks.x_2_3.conv1.1.running_var\", \"decoder.blocks.x_2_3.conv2.0.weight\", \"decoder.blocks.x_2_3.conv2.1.weight\", \"decoder.blocks.x_2_3.conv2.1.bias\", \"decoder.blocks.x_2_3.conv2.1.running_mean\", \"decoder.blocks.x_2_3.conv2.1.running_var\", \"decoder.blocks.x_3_3.conv1.0.weight\", \"decoder.blocks.x_3_3.conv1.1.weight\", \"decoder.blocks.x_3_3.conv1.1.bias\", \"decoder.blocks.x_3_3.conv1.1.running_mean\", \"decoder.blocks.x_3_3.conv1.1.running_var\", \"decoder.blocks.x_3_3.conv2.0.weight\", \"decoder.blocks.x_3_3.conv2.1.weight\", \"decoder.blocks.x_3_3.conv2.1.bias\", \"decoder.blocks.x_3_3.conv2.1.running_mean\", \"decoder.blocks.x_3_3.conv2.1.running_var\", \"decoder.blocks.x_0_4.conv1.0.weight\", \"decoder.blocks.x_0_4.conv1.1.weight\", \"decoder.blocks.x_0_4.conv1.1.bias\", \"decoder.blocks.x_0_4.conv1.1.running_mean\", \"decoder.blocks.x_0_4.conv1.1.running_var\", \"decoder.blocks.x_0_4.conv2.0.weight\", \"decoder.blocks.x_0_4.conv2.1.weight\", \"decoder.blocks.x_0_4.conv2.1.bias\", \"decoder.blocks.x_0_4.conv2.1.running_mean\", \"decoder.blocks.x_0_4.conv2.1.running_var\". \n\tUnexpected key(s) in state_dict: \"decoder.0.convs.0.0.weight\", \"decoder.0.convs.0.1.weight\", \"decoder.0.convs.0.1.bias\", \"decoder.0.convs.0.1.running_mean\", \"decoder.0.convs.0.1.running_var\", \"decoder.0.convs.0.1.num_batches_tracked\", \"decoder.0.convs.1.0.weight\", \"decoder.0.convs.1.1.weight\", \"decoder.0.convs.1.1.bias\", \"decoder.0.convs.1.1.running_mean\", \"decoder.0.convs.1.1.running_var\", \"decoder.0.convs.1.1.num_batches_tracked\", \"decoder.0.convs.2.0.weight\", \"decoder.0.convs.2.1.weight\", \"decoder.0.convs.2.1.bias\", \"decoder.0.convs.2.1.running_mean\", \"decoder.0.convs.2.1.running_var\", \"decoder.0.convs.2.1.num_batches_tracked\", \"decoder.0.convs.3.0.weight\", \"decoder.0.convs.3.1.weight\", \"decoder.0.convs.3.1.bias\", \"decoder.0.convs.3.1.running_mean\", \"decoder.0.convs.3.1.running_var\", \"decoder.0.convs.3.1.num_batches_tracked\", \"decoder.0.convs.4.1.weight\", \"decoder.0.convs.4.2.weight\", \"decoder.0.convs.4.2.bias\", \"decoder.0.convs.4.2.running_mean\", \"decoder.0.convs.4.2.running_var\", \"decoder.0.convs.4.2.num_batches_tracked\", \"decoder.0.project.0.weight\", \"decoder.0.project.1.weight\", \"decoder.0.project.1.bias\", \"decoder.0.project.1.running_mean\", \"decoder.0.project.1.running_var\", \"decoder.0.project.1.num_batches_tracked\", \"decoder.1.weight\", \"decoder.2.weight\", \"decoder.2.bias\", \"decoder.2.running_mean\", \"decoder.2.running_var\", \"decoder.2.num_batches_tracked\". \n\tsize mismatch for segmentation_head.0.weight: copying a param with shape torch.Size([1, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 16, 3, 3]).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a3c7589822a7>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive/MyDrive/ML/predictions/U_NET++_1_base_case_100_im.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive/MyDrive/ML/predictions/DEEPLAB_1_base_case_100_im.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2584\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2585\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2586\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for UnetPlusPlus:\n\tMissing key(s) in state_dict: \"decoder.blocks.x_0_0.conv1.0.weight\", \"decoder.blocks.x_0_0.conv1.1.weight\", \"decoder.blocks.x_0_0.conv1.1.bias\", \"decoder.blocks.x_0_0.conv1.1.running_mean\", \"decoder.blocks.x_0_0.conv1.1.running_var\", \"decoder.blocks.x_0_0.conv2.0.weight\", \"decoder.blocks.x_0_0.conv2.1.weight\", \"decoder.blocks.x_0_0.conv2.1.bias\", \"decoder.blocks.x_0_0.conv2.1.running_mean\", \"decoder.blocks.x_0_0.conv2.1.running_var\", \"decoder.blocks.x_0_1.conv1.0.weight\", \"decoder.blocks.x_0_1.conv1.1.weight\", \"decoder.blocks.x_0_1.conv1.1.bias\", \"decoder.blocks.x_0_1.conv1.1.running_mean\", \"decoder.blocks.x_0_1.conv1.1.running_var\", \"decoder.blocks.x_0_1.conv2.0.weight\", \"decoder.blocks.x_0_1.conv2.1.weight\", \"decoder.blocks.x_0_1.conv2.1.bias\", \"decoder.blocks.x_0_1.conv2.1.running_mean\", \"decoder.blocks.x_0_1.conv2.1.running_var\", \"decoder.blocks.x_1_1.conv1.0.weight\", \"decoder.blocks.x_1_1.conv1.1.weight\", \"decoder.blocks.x_1_1.conv1.1.bias\", \"decoder.blocks.x_1_1.conv1.1.running_mean\", \"decoder.blocks.x_1_1.conv1.1.running_var\", \"decoder.blocks.x_1_1.conv2.0.weight\", \"decoder.blocks.x_1_1.conv2.1.weight\", \"decoder.blocks.x_1_1.conv2.1.bias\", \"decoder.blocks.x_1_1.conv2.1.running_mean\", \"decoder.blocks.x_1_1.conv2.1.running_var\", \"decoder.blocks.x_0_2.conv1.0.weight\", \"decoder.blocks.x_0_2.conv1.1.weight\", \"decoder.blocks.x_0_2.conv1.1.bias\", \"decoder.blocks.x_0_2.conv1.1.running_mean\", \"decoder.blocks.x_0_2.conv1.1.running_var\", \"decoder.blocks.x_0_2.conv2.0.weight\",...\n\tUnexpected key(s) in state_dict: \"decoder.0.convs.0.0.weight\", \"decoder.0.convs.0.1.weight\", \"decoder.0.convs.0.1.bias\", \"decoder.0.convs.0.1.running_mean\", \"decoder.0.convs.0.1.running_var\", \"decoder.0.convs.0.1.num_batches_tracked\", \"decoder.0.convs.1.0.weight\", \"decoder.0.convs.1.1.weight\", \"decoder.0.convs.1.1.bias\", \"decoder.0.convs.1.1.running_mean\", \"decoder.0.convs.1.1.running_var\", \"decoder.0.convs.1.1.num_batches_tracked\", \"decoder.0.convs.2.0.weight\", \"decoder.0.convs.2.1.weight\", \"decoder.0.convs.2.1.bias\", \"decoder.0.convs.2.1.running_mean\", \"decoder.0.convs.2.1.running_var\", \"decoder.0.convs.2.1.num_batches_tracked\", \"decoder.0.convs.3.0.weight\", \"decoder.0.convs.3.1.weight\", \"decoder.0.convs.3.1.bias\", \"decoder.0.convs.3.1.running_mean\", \"decoder.0.convs.3.1.running_var\", \"decoder.0.convs.3.1.num_batches_tracked\", \"decoder.0.convs.4.1.weight\", \"decoder.0.convs.4.2.weight\", \"decoder.0.convs.4.2.bias\", \"decoder.0.convs.4.2.running_mean\", \"decoder.0.convs.4.2.running_var\", \"decoder.0.convs.4.2.num_batches_tracked\", \"decoder.0.project.0.weight\", \"decoder.0.project.1.weight\", \"decoder.0.project.1.bias\", \"decoder.0.project.1.running_mean\", \"decoder.0.project.1.running_var\", \"decoder.0.project.1.num_batches_tracked\", \"decoder.1.weight\", \"decoder.2.weight\", \"decoder.2.bias\", \"decoder.2.running_mean\", \"decoder.2.running_var\", \"decoder.2.num_batches_tracked\". \n\tsize mismatch for segmentation_head.0.weight: copying a param with shape torch.Size([1, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 16, 3, 3])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vsSF1FAm1Kwu"
      },
      "id": "vsSF1FAm1Kwu",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}