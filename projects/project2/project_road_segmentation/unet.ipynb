{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id",
        "outputId": "9450cf42-1e7f-4f9f-a513-dea5fb720970",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation_models_pytorch\n",
            "  Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting efficientnet-pytorch==0.7.1 (from segmentation_models_pytorch)\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: huggingface-hub>=0.24.6 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.26.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (11.0.0)\n",
            "Collecting pretrainedmodels==0.7.4 (from segmentation_models_pytorch)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.17.0)\n",
            "Collecting timm==0.9.7 (from segmentation_models_pytorch)\n",
            "  Downloading timm-0.9.7-py3-none-any.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (4.66.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.5.1+cu121)\n",
            "Collecting munch (from pretrainedmodels==0.7.4->segmentation_models_pytorch)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation_models_pytorch) (6.0.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation_models_pytorch) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (4.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.26.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.0.2)\n",
            "Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=59285ede200df968c8426b914d53dba06a248f008de272c714d6afff5c34e0c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=688de6b0d64343046273b5f3e0a232d1f9c09acb786db4ae038895f10360b4bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation_models_pytorch\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.12\n",
            "    Uninstalling timm-1.0.12:\n",
            "      Successfully uninstalled timm-1.0.12\n",
            "Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.3.4 timm-0.9.7\n",
            "Mounted at /content/gdrive\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.9 torchmetrics-1.6.0\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from functools import partial\n",
        "import matplotlib\n",
        "import matplotlib.image as mpimg\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "from google.colab import drive\n",
        "!pip install segmentation_models_pytorch\n",
        "drive.mount('/content/gdrive')\n",
        "import seaborn as sns\n",
        "import segmentation_models_pytorch as smp\n",
        "from segmentation_models_pytorch import Unet, UnetPlusPlus, DeepLabV3\n",
        "import segmentation_models_pytorch\n",
        "!pip install torchmetrics\n",
        "from torchmetrics.functional import f1_score\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ]
    },
    {
      "metadata": {
        "id": "ec3b705ceb1aebdd"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 2,
      "source": [
        "###\n",
        "###\n",
        "# Paths\n",
        "root_dir = \"/content/gdrive/MyDrive/ML/data/original/\"\n",
        "image_dir = root_dir + \"images/\"\n",
        "gt_dir = root_dir + \"groundtruth\"\n",
        "test_image_dir = \"data/test_set_images_gradient/\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "id": "ec3b705ceb1aebdd"
    },
    {
      "metadata": {
        "id": "df74a707ef0c45a8"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 14,
      "source": [
        "###\n",
        "###\n",
        "# Hyper parameters\n",
        "PADDING = 0\n",
        "EPOCH = 15\n",
        "BATCH_SIZE = 10\n",
        "NR_IMAGES = 100\n",
        "IMAGE_SIZE = 256"
      ],
      "id": "df74a707ef0c45a8"
    },
    {
      "cell_type": "code",
      "source": [
        "class PreloadedDataset(Dataset):\n",
        "    def __init__(self, images, ground_truth):\n",
        "        self.images = images\n",
        "        self.ground_truth = ground_truth\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        ground_truth = self.ground_truth[idx]\n",
        "        return image, ground_truth\n",
        "\n",
        "\n",
        "def split(dataset, tr,val):\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [tr, val])\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "def get_dataloaders(train_dataset, val_dataset, batch_size):\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def load_npy(npy_file_path):\n",
        "  array = np.load(npy_file_path)\n",
        "  return array\n",
        "\n",
        "def load_image(infilename):\n",
        "    data = mpimg.imread(infilename)\n",
        "    return data\n",
        "\n",
        "def get_image_name(image_dir):\n",
        "    file_name = os.path.basename(image_dir)\n",
        "    return file_name\n",
        "\n",
        "def img_crop(im, w, h):\n",
        "    list_patches = []\n",
        "    imgwidth = im.shape[0]\n",
        "    imgheight = im.shape[1]\n",
        "\n",
        "    is_2d = len(im.shape) < 3\n",
        "    for i in range(0, imgheight, h):\n",
        "        for j in range(0, imgwidth, w):\n",
        "            if is_2d:\n",
        "                im_patch = im[j : j + w, i : i + h]\n",
        "            else:\n",
        "                im_patch = im[j : j + w, i : i + h, :]\n",
        "            list_patches.append(im_patch)\n",
        "    return list_patches\n",
        "\n",
        "def predict_patches(prediction, patch_threshold):\n",
        "    patches = img_crop(prediction, 16, 16)\n",
        "    iter = int(np.sqrt(len(patches)))\n",
        "    results = np.zeros((iter, iter))\n",
        "    c = 0\n",
        "    for j in range(iter):\n",
        "      for i in range(iter):\n",
        "        if patches[c].mean() > patch_threshold:\n",
        "          results[i, j] = 1\n",
        "        c = c + 1\n",
        "    return results\n",
        "\n",
        "def masks_to_submission(submission_filename, results):\n",
        "    \"\"\"Converts images into a submission file\"\"\"\n",
        "    img_number = 0\n",
        "    with open(submission_filename, 'w') as f:\n",
        "        f.write('id,prediction\\n')\n",
        "        for res in results:\n",
        "            img_number = img_number + 1\n",
        "            f.writelines('{}\\n'.format(s) for s in mask_to_submission_strings(res, img_number))\n",
        "\n",
        "def mask_to_submission_strings(result, img_number):\n",
        "    \"\"\"Reads a single image and outputs the strings that should go into the submission file\"\"\"\n",
        "    im = result\n",
        "    patch_size = 16\n",
        "    for j in range(0, im.shape[1]):\n",
        "        for i in range(0, im.shape[0]):\n",
        "            label = result[i, j]\n",
        "\n",
        "            yield(\"{:03d}_{}_{},{}\".format(img_number, j*16, i*16, int(label)))\n",
        "\n",
        "def write_predictions_to_file(predictions, labels, filename):\n",
        "    max_labels = np.argmax(labels, 1)\n",
        "    max_predictions = np.argmax(predictions, 1)\n",
        "    file = open(filename, \"w\")\n",
        "    n = predictions.shape[0]\n",
        "    for i in range(0, n):\n",
        "        file.write(max_labels(i) + \" \" + max_predictions(i))\n",
        "    file.close()\n",
        "\n",
        "def compute_f1(res, ans):\n",
        "    \"\"\"\n",
        "    Precision: tp/(tp+fp)\n",
        "    Recall: tp/(tp+fn)\n",
        "    F1 = 2 * (precision * recall) / (precision + recall)\n",
        "    \"\"\"\n",
        "\n",
        "    tp = torch.sum((res == 1) & (ans == 1))\n",
        "    fp = torch.sum((res == 1) & (ans != 1))\n",
        "    fn = torch.sum((res != 1) & (ans == 1))\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) != 0 else torch.tensor(0.0)\n",
        "    recall = tp / (tp + fn) if (tp + fn) != 0 else torch.tensor(0.0)\n",
        "    if precision + recall == 0:\n",
        "        return torch.tensor(0.0)\n",
        "    else:\n",
        "        return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "\n",
        "def get_threshold(predictions,labels):\n",
        "    f1_max = 0\n",
        "    th = 0\n",
        "\n",
        "    for t in range(0,100):\n",
        "        t = t/100\n",
        "        th_pred = predictions.clone()\n",
        "        patches = predict_patches(th_pred, t)\n",
        "        f1 = f1_score(torch.tensor(patches), torch.tensor(labels), task='binary')\n",
        "\n",
        "        if f1>f1_max:\n",
        "            f1_max = f1\n",
        "            th = t\n",
        "    return th\n",
        "\n",
        "def train_epoch(model, optimizer, scheduler, criterion, train_loader, epoch, device):\n",
        "    model.train()\n",
        "\n",
        "    # Use tensors for histories\n",
        "    total_loss = torch.tensor(0.0, device=device)\n",
        "    total_samples = 0\n",
        "    total_accuracy = torch.tensor(0.0, device=device)\n",
        "    total_f1 = torch.tensor(0.0, device=device)\n",
        "    batch_iter = 0\n",
        "    for batch_idx, (batch_data, batch_labels) in enumerate(train_loader):\n",
        "\n",
        "        # Prepare batch\n",
        "        batch_data = batch_data.to(device)\n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(batch_data)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, batch_labels)\n",
        "        total_loss = total_loss + loss\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute accuracy and F1 per batch\n",
        "        predictions = (output > 0.3).int()  # Hardcoded threshold\n",
        "        batch_correct = (predictions == batch_labels).sum()\n",
        "\n",
        "        batch_accuracy = batch_correct / (batch_labels.shape[0] * batch_labels.shape[1] * batch_labels.shape[2] * batch_labels.shape[3])\n",
        "        total_accuracy = total_accuracy + batch_accuracy\n",
        "\n",
        "        total_f1 = total_f1 + f1_score(predictions, batch_labels, task='binary')\n",
        "\n",
        "\n",
        "\n",
        "        batch_iter = batch_iter + 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Compute averages for the epoch\n",
        "    epoch_loss = total_loss / batch_iter\n",
        "    epoch_accuracy = total_accuracy / batch_iter\n",
        "    epoch_f1 = total_f1 / batch_iter\n",
        "\n",
        "    # Log the learning rate history\n",
        "    lr_value = scheduler.get_last_lr()[0]\n",
        "\n",
        "    print(\n",
        "        f\"Epoch Summary: {epoch} \"\n",
        "        f\"Epoch Loss={epoch_loss.item():.3e} \"\n",
        "        f\"Accuracy={epoch_accuracy.item():.3f} \"\n",
        "        f\"F1={epoch_f1.item():.3e} \"\n",
        "        f\"LR={lr_value:.3e}\"\n",
        "    )\n",
        "\n",
        "    # Return metrics as tensors\n",
        "    return epoch_loss, epoch_accuracy, lr_value, epoch_f1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, device, val_loader, criterion):\n",
        "    model.eval()  # Important: eval mode (affects dropout, batch norm, etc.)\n",
        "\n",
        "    # Initialize tensors for aggregating metrics\n",
        "    total_loss = torch.tensor(0.0, device=device)\n",
        "    total_accuracy = torch.tensor(0.0, device=device)\n",
        "    total_f1 = torch.tensor(0.0, device=device)\n",
        "\n",
        "\n",
        "    batch_iter = 0\n",
        "    for data, target in val_loader:\n",
        "        # Prepare batch\n",
        "\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "\n",
        "\n",
        "\n",
        "        # Compute loss\n",
        "        batch_loss = criterion(output, target)\n",
        "        total_loss = total_loss +  batch_loss\n",
        "\n",
        "        # Compute predictions\n",
        "        probabilities = F.sigmoid(output)\n",
        "        predictions = (probabilities > 0.3).int()  # Hardcoded threshold\n",
        "        total_f1 = total_f1 + compute_f1(predictions, target)\n",
        "\n",
        "        # F1 and accuracy\n",
        "        batch_correct = (predictions == target).sum()\n",
        "        batch_accuracy = batch_correct / (target.shape[0] * target.shape[1] * target.shape[2] * target.shape[3])\n",
        "\n",
        "        total_accuracy = total_accuracy + batch_accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        batch_iter = batch_iter + 1\n",
        "\n",
        "    # Compute averages\n",
        "    avg_loss = total_loss / batch_iter\n",
        "    avg_accuracy = total_accuracy / batch_iter\n",
        "    avg_f1 = total_f1 / batch_iter\n",
        "\n",
        "    # Print validation summary\n",
        "    print(\n",
        "        f\"Validation Set: Average Loss: {avg_loss.item():.4f}, \"\n",
        "        f\"Accuracy: {avg_accuracy.item() * 100:.2f}%, \"\n",
        "        f\"F1: {avg_f1.item():.4f}, \"\n",
        "    )\n",
        "\n",
        "    # Return metrics as tensors\n",
        "    return avg_loss, avg_f1, avg_accuracy * 100\n",
        "\n",
        "\n",
        "def run_training(\n",
        "    model,\n",
        "    num_epochs,\n",
        "    optimizer_kwargs,\n",
        "    data_kwargs,\n",
        "    device=\"cuda\",\n",
        "\n",
        "):\n",
        "    # ===== Data Loading =====\n",
        "    train_loader, val_loader = get_dataloaders(**data_kwargs)\n",
        "\n",
        "    # ===== Model, Optimizer and Criterion =====\n",
        "    model = model.to(device=device)\n",
        "    # model.apply(init_weights)\n",
        "    #optimizer = torch.optim.AdamW(model.parameters(), **optimizer_kwargs)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), **optimizer_kwargs)\n",
        "    criterion = segmentation_models_pytorch.losses.DiceLoss(\"binary\")\n",
        "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.90)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "\n",
        "    # ===== Train Model =====\n",
        "    lr_history = []\n",
        "    train_loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_loss_history = []\n",
        "    val_acc_history = []\n",
        "    val_f1_history = []\n",
        "    train_f1_history = []\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f'Epoch {epoch}/{num_epochs}')\n",
        "        train_loss, train_acc, lrs, f1_hist = train_epoch(\n",
        "            model, optimizer, scheduler, criterion, train_loader, epoch, device\n",
        "        )\n",
        "        train_loss_history.append(train_loss)\n",
        "        train_acc_history.append(train_acc)\n",
        "        lr_history.append(lrs)\n",
        "\n",
        "        val_loss, f1, val_acc = validate(model, device, val_loader, criterion)\n",
        "        val_loss_history.append(val_loss)\n",
        "        val_f1_history.append(f1)\n",
        "        val_acc_history.append(val_acc)\n",
        "        train_f1_history.append(f1_hist)\n",
        "        scheduler.step(train_loss)\n",
        "\n",
        "    return train_loss_history, train_acc_history, val_loss_history, val_f1_history, val_acc_history, train_f1_history\n"
      ],
      "metadata": {
        "id": "LkDa9-hc4tx4"
      },
      "id": "LkDa9-hc4tx4",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "UNet implementation.\n",
        "\"\"\"\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    UNet neural network.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int = 3,\n",
        "        out_channels: int = 1,\n",
        "        init_features: int = 32,\n",
        "        dropout: bool = False,\n",
        "        prob: float = 0.2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        features = init_features\n",
        "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.bottleneck = UNet._block(\n",
        "            features * 8, features * 16,\n",
        "            name=\"bottleneck\"\n",
        "        )\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(\n",
        "            features * 16, features * 8, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder4 = UNet._block(\n",
        "            (features * 8) * 2, features * 8,\n",
        "            name=\"dec4\"\n",
        "        )\n",
        "        self.upconv3 = nn.ConvTranspose2d(\n",
        "            features * 8, features * 4, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder3 = UNet._block(\n",
        "            (features * 4) * 2, features * 4,\n",
        "            name=\"dec3\"\n",
        "        )\n",
        "        self.upconv2 = nn.ConvTranspose2d(\n",
        "            features * 4, features * 2, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder2 = UNet._block(\n",
        "            (features * 2) * 2, features * 2,\n",
        "            name=\"dec2\"\n",
        "        )\n",
        "        self.upconv1 = nn.ConvTranspose2d(\n",
        "            features * 2, features, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
        "\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=features, out_channels=out_channels, kernel_size=1\n",
        "        )\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = dropout\n",
        "        if self.dropout:\n",
        "            self.drop = nn.Dropout(p=prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.encoder1(x)\n",
        "        pool1 = self.pool1(enc1)\n",
        "        if self.dropout:\n",
        "            pool1 = self.drop(pool1)\n",
        "        enc2 = self.encoder2(pool1)\n",
        "        pool2 = self.pool2(enc2)\n",
        "        if self.dropout:\n",
        "            pool2 = self.drop(pool2)\n",
        "        enc3 = self.encoder3(pool2)\n",
        "        pool3 = self.pool3(enc3)\n",
        "        if self.dropout:\n",
        "            pool3 = self.drop(pool3)\n",
        "        enc4 = self.encoder4(pool3)\n",
        "\n",
        "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
        "\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
        "        if self.dropout:\n",
        "            dec4 = self.drop(dec4)\n",
        "        dec4 = self.decoder4(dec4)\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
        "        if self.dropout:\n",
        "            dec3 = self.drop(dec3)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
        "        if self.dropout:\n",
        "            dec2 = self.drop(dec2)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
        "        if self.dropout:\n",
        "            dec1 = self.drop(dec1)\n",
        "        dec1 = self.decoder1(dec1)\n",
        "        return torch.sigmoid(self.conv(dec1))\n",
        "\n",
        "    @staticmethod\n",
        "    def _block(in_channels, features, name):\n",
        "        return nn.Sequential(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    (\n",
        "                        name + \"conv1\",\n",
        "                        nn.Conv2d(\n",
        "                            in_channels=in_channels,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
        "                    (\n",
        "                        name + \"conv2\",\n",
        "                        nn.Conv2d(\n",
        "                            in_channels=features,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
        "                ]\n",
        "            )\n",
        "        )"
      ],
      "metadata": {
        "id": "8IAriqGJfQJ_"
      },
      "id": "8IAriqGJfQJ_",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "###\n",
        "# Loading data set\n",
        "\n",
        "\n",
        "imgs = []\n",
        "gt_imgs = []\n",
        "image_files = sorted(os.listdir(image_dir))[:NR_IMAGES]\n",
        "gt_files = sorted(os.listdir(gt_dir))[:NR_IMAGES]\n",
        "org_len = len(gt_files)\n",
        "img_nr = 0\n",
        "\n",
        "for image_file in image_files:\n",
        "    # Removing from image name (extended)\n",
        "    image_name = os.path.splitext(image_file)[0]#[:-10]\n",
        "    matching_gt_file = None\n",
        "\n",
        "    gt_match_index = 0\n",
        "    for gt_file in gt_files:\n",
        "\n",
        "        gt_name = os.path.splitext(gt_file)[0]\n",
        "\n",
        "        if gt_name[-4:] == '.png':\n",
        "            gt_name = gt_name[:-4]\n",
        "        if gt_name == image_name:\n",
        "            matching_gt_file = gt_file\n",
        "            img_nr = img_nr + 1\n",
        "            print(f'{img_nr} / {org_len} loaded')\n",
        "            break\n",
        "\n",
        "    if matching_gt_file:\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "        gt_path = os.path.join(gt_dir, matching_gt_file)\n",
        "\n",
        "        image = load_image(image_path)\n",
        "        gt_image = load_image(gt_path)\n",
        "        imgs.append(image[:256, :256,:])\n",
        "        imgs.append(image[144:, :256,:])\n",
        "        imgs.append(image[:256, 144:,:])\n",
        "        imgs.append(image[144:, 144:,:])\n",
        "\n",
        "        gt_imgs.append(gt_image[:256, :256])\n",
        "        gt_imgs.append(gt_image[144:, :256])\n",
        "        gt_imgs.append(gt_image[:256, 144:])\n",
        "        gt_imgs.append(gt_image[144:, 144:])\n"
      ],
      "metadata": {
        "id": "GXLYYNem7xrL",
        "outputId": "75180ed6-d46a-4348-af48-8589fc3f8ba3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "GXLYYNem7xrL",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 / 100 loaded\n",
            "2 / 100 loaded\n",
            "3 / 100 loaded\n",
            "4 / 100 loaded\n",
            "5 / 100 loaded\n",
            "6 / 100 loaded\n",
            "7 / 100 loaded\n",
            "8 / 100 loaded\n",
            "9 / 100 loaded\n",
            "10 / 100 loaded\n",
            "11 / 100 loaded\n",
            "12 / 100 loaded\n",
            "13 / 100 loaded\n",
            "14 / 100 loaded\n",
            "15 / 100 loaded\n",
            "16 / 100 loaded\n",
            "17 / 100 loaded\n",
            "18 / 100 loaded\n",
            "19 / 100 loaded\n",
            "20 / 100 loaded\n",
            "21 / 100 loaded\n",
            "22 / 100 loaded\n",
            "23 / 100 loaded\n",
            "24 / 100 loaded\n",
            "25 / 100 loaded\n",
            "26 / 100 loaded\n",
            "27 / 100 loaded\n",
            "28 / 100 loaded\n",
            "29 / 100 loaded\n",
            "30 / 100 loaded\n",
            "31 / 100 loaded\n",
            "32 / 100 loaded\n",
            "33 / 100 loaded\n",
            "34 / 100 loaded\n",
            "35 / 100 loaded\n",
            "36 / 100 loaded\n",
            "37 / 100 loaded\n",
            "38 / 100 loaded\n",
            "39 / 100 loaded\n",
            "40 / 100 loaded\n",
            "41 / 100 loaded\n",
            "42 / 100 loaded\n",
            "43 / 100 loaded\n",
            "44 / 100 loaded\n",
            "45 / 100 loaded\n",
            "46 / 100 loaded\n",
            "47 / 100 loaded\n",
            "48 / 100 loaded\n",
            "49 / 100 loaded\n",
            "50 / 100 loaded\n",
            "51 / 100 loaded\n",
            "52 / 100 loaded\n",
            "53 / 100 loaded\n",
            "54 / 100 loaded\n",
            "55 / 100 loaded\n",
            "56 / 100 loaded\n",
            "57 / 100 loaded\n",
            "58 / 100 loaded\n",
            "59 / 100 loaded\n",
            "60 / 100 loaded\n",
            "61 / 100 loaded\n",
            "62 / 100 loaded\n",
            "63 / 100 loaded\n",
            "64 / 100 loaded\n",
            "65 / 100 loaded\n",
            "66 / 100 loaded\n",
            "67 / 100 loaded\n",
            "68 / 100 loaded\n",
            "69 / 100 loaded\n",
            "70 / 100 loaded\n",
            "71 / 100 loaded\n",
            "72 / 100 loaded\n",
            "73 / 100 loaded\n",
            "74 / 100 loaded\n",
            "75 / 100 loaded\n",
            "76 / 100 loaded\n",
            "77 / 100 loaded\n",
            "78 / 100 loaded\n",
            "79 / 100 loaded\n",
            "80 / 100 loaded\n",
            "81 / 100 loaded\n",
            "82 / 100 loaded\n",
            "83 / 100 loaded\n",
            "84 / 100 loaded\n",
            "85 / 100 loaded\n",
            "86 / 100 loaded\n",
            "87 / 100 loaded\n",
            "88 / 100 loaded\n",
            "89 / 100 loaded\n",
            "90 / 100 loaded\n",
            "91 / 100 loaded\n",
            "92 / 100 loaded\n",
            "93 / 100 loaded\n",
            "94 / 100 loaded\n",
            "95 / 100 loaded\n",
            "96 / 100 loaded\n",
            "97 / 100 loaded\n",
            "98 / 100 loaded\n",
            "99 / 100 loaded\n",
            "100 / 100 loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting to float\n",
        "imgs = np.array([sub_arr.astype(np.float32) for sub_arr in imgs])\n",
        "gt_imgs = np.array([sub_arr.astype(np.float32) for sub_arr in gt_imgs])\n",
        "\n",
        "# Making sure gt is binary, road or no road\n",
        "gt_imgs[gt_imgs > 0] = 1"
      ],
      "metadata": {
        "id": "8Us3mN6-7zJN"
      },
      "id": "8Us3mN6-7zJN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting to tensors\n",
        "gt_imgs = torch.tensor(gt_imgs)\n",
        "imgs = torch.tensor(imgs)"
      ],
      "metadata": {
        "id": "g2W6ABp372OL"
      },
      "id": "g2W6ABp372OL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs.shape\n",
        "imgs = imgs.permute(0, 3, 1, 2)"
      ],
      "metadata": {
        "id": "w7yI9Kz2E6Kk"
      },
      "id": "w7yI9Kz2E6Kk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gt_imgs.shape"
      ],
      "metadata": {
        "id": "b2LudyREwhSh",
        "outputId": "89921ec1-acab-4632-80ec-c980a2ccc69c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "b2LudyREwhSh",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([400, 256, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gt_imgs = gt_imgs.reshape(gt_imgs.shape[0], 1, 256, 256)"
      ],
      "metadata": {
        "id": "nKIas_VnERd-"
      },
      "id": "nKIas_VnERd-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading train_loader and val_loader\n",
        "dataset = PreloadedDataset(images = imgs, ground_truth = gt_imgs)\n",
        "train_dataset, val_dataset = split(dataset, 0.8, 0.2)"
      ],
      "metadata": {
        "id": "ljbENi9A7-lU"
      },
      "id": "ljbENi9A7-lU",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = EPOCH\n",
        "\n",
        "learning_rates = [1e-4]\n",
        "weight_decays = [1e-4]\n",
        "\n",
        "\n",
        "data_kwargs = dict(\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset = val_dataset,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# For Hyperparameter fitting\n",
        "combinations = []\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_f1 = []\n",
        "val_f1 = []\n",
        "\n",
        "best_loss = 20\n",
        "for l in learning_rates:\n",
        "  for wd in weight_decays:\n",
        "#       model = Unet(\n",
        "#       encoder_name=\"vgg11\",        # Choose encoder\n",
        "#       encoder_weights=None,    # Use pre-trained ImageNet weights\n",
        "#       classes=1,                     # Number of output classes\n",
        "#       activation=None               # No activation, as it's handled in loss/metrics\n",
        "# )\n",
        "      model = UNet()\n",
        "      optimizer_kwargs = dict(\n",
        "      lr=l,\n",
        "      weight_decay=wd,\n",
        "      )\n",
        "      train_loss, train_acc_history, val_loss_history, val_f1_history, val_acc_history, train_f1_history = run_training(\n",
        "          model=model,\n",
        "          num_epochs=num_epochs,\n",
        "          optimizer_kwargs=optimizer_kwargs,\n",
        "          data_kwargs=data_kwargs,\n",
        "          device=device,\n",
        "          )\n",
        "      combination = 'learning_rate_' +  str(l) + '_weight_decay_' + str(wd)\n",
        "      combinations.append(combination)\n",
        "\n",
        "      train_f1.append(train_f1_history)\n",
        "      val_f1.append(val_f1_history)\n",
        "      train_losses.append(train_loss)\n",
        "      val_losses.append(val_loss_history)\n",
        "\n",
        "      if train_loss[-1] < best_loss:\n",
        "        best_loss = train_loss[-1]\n",
        "        best_pair = (l,wd)\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), \"U_NET_1_base_case_100_im.pth\")"
      ],
      "metadata": {
        "id": "MwY5rYGP8Q9E",
        "outputId": "5b7dc4d8-8710-4e11-e5b1-1eecf1d773b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MwY5rYGP8Q9E",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "Epoch Summary: 1 Epoch Loss=6.659e-01 Accuracy=0.231 F1=3.710e-01 LR=1.000e-04\n",
            "Validation Set: Average Loss: 0.6896, Accuracy: 20.65%, F1: 0.3418, \n",
            "Epoch 2/15\n",
            "Epoch Summary: 2 Epoch Loss=6.662e-01 Accuracy=0.231 F1=3.707e-01 LR=1.000e-04\n",
            "Validation Set: Average Loss: 0.6896, Accuracy: 20.65%, F1: 0.3418, \n",
            "Epoch 3/15\n",
            "Epoch Summary: 3 Epoch Loss=6.666e-01 Accuracy=0.231 F1=3.703e-01 LR=1.000e-04\n",
            "Validation Set: Average Loss: 0.6895, Accuracy: 20.65%, F1: 0.3418, \n",
            "Epoch 4/15\n",
            "Epoch Summary: 4 Epoch Loss=6.661e-01 Accuracy=0.231 F1=3.707e-01 LR=1.000e-04\n",
            "Validation Set: Average Loss: 0.6895, Accuracy: 20.65%, F1: 0.3418, \n",
            "Epoch 5/15\n",
            "Epoch Summary: 5 Epoch Loss=6.656e-01 Accuracy=0.231 F1=3.712e-01 LR=1.000e-04\n",
            "Validation Set: Average Loss: 0.6895, Accuracy: 20.65%, F1: 0.3418, \n",
            "Epoch 6/15\n",
            "Epoch Summary: 6 Epoch Loss=6.658e-01 Accuracy=0.231 F1=3.710e-01 LR=1.000e-04\n",
            "Validation Set: Average Loss: 0.6895, Accuracy: 20.65%, F1: 0.3418, \n",
            "Epoch 7/15\n",
            "Epoch Summary: 7 Epoch Loss=6.658e-01 Accuracy=0.231 F1=3.711e-01 LR=1.000e-04\n",
            "Validation Set: Average Loss: 0.6895, Accuracy: 20.65%, F1: 0.3418, \n",
            "Epoch 8/15\n",
            "Epoch Summary: 8 Epoch Loss=6.660e-01 Accuracy=0.231 F1=3.708e-01 LR=1.000e-04\n",
            "Validation Set: Average Loss: 0.6895, Accuracy: 20.65%, F1: 0.3418, \n",
            "Epoch 9/15\n",
            "Epoch Summary: 9 Epoch Loss=6.659e-01 Accuracy=0.231 F1=3.709e-01 LR=1.000e-04\n",
            "Validation Set: Average Loss: 0.6895, Accuracy: 20.65%, F1: 0.3418, \n",
            "Epoch 10/15\n",
            "Epoch Summary: 10 Epoch Loss=6.657e-01 Accuracy=0.231 F1=3.710e-01 LR=1.000e-04\n",
            "Validation Set: Average Loss: 0.6894, Accuracy: 20.65%, F1: 0.3418, \n",
            "Epoch 11/15\n",
            "Epoch Summary: 11 Epoch Loss=6.658e-01 Accuracy=0.231 F1=3.710e-01 LR=1.000e-04\n",
            "Validation Set: Average Loss: 0.6894, Accuracy: 20.65%, F1: 0.3418, \n",
            "Epoch 12/15\n",
            "Epoch Summary: 12 Epoch Loss=6.657e-01 Accuracy=0.231 F1=3.710e-01 LR=1.000e-05\n",
            "Validation Set: Average Loss: 0.6894, Accuracy: 20.65%, F1: 0.3418, \n",
            "Epoch 13/15\n",
            "Epoch Summary: 13 Epoch Loss=6.656e-01 Accuracy=0.231 F1=3.712e-01 LR=1.000e-05\n",
            "Validation Set: Average Loss: 0.6894, Accuracy: 20.65%, F1: 0.3418, \n",
            "Epoch 14/15\n",
            "Epoch Summary: 14 Epoch Loss=6.658e-01 Accuracy=0.231 F1=3.710e-01 LR=1.000e-05\n",
            "Validation Set: Average Loss: 0.6895, Accuracy: 20.65%, F1: 0.3418, \n",
            "Epoch 15/15\n",
            "Epoch Summary: 15 Epoch Loss=6.664e-01 Accuracy=0.231 F1=3.705e-01 LR=1.000e-05\n",
            "Validation Set: Average Loss: 0.6894, Accuracy: 20.65%, F1: 0.3418, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_colors = 9\n",
        "colors = plt.cm.tab20c(np.linspace(0, 1, n_colors))\n",
        "\n",
        "train_losses = [[loss.cpu().detach().numpy() for loss in loss_param] for loss_param in train_losses]\n",
        "val_losses = [[loss.cpu().detach().numpy() for loss in loss_param] for loss_param in val_losses]\n",
        "\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "print(len(train_losses[0]))\n",
        "iterations = len(learning_rates) * len(weight_decays)\n",
        "for i in range(iterations):\n",
        "    train_losses[i] = [entry for entry in train_losses[i]]\n",
        "    axes[0].plot(train_losses[i], label=combinations[i])\n",
        "\n",
        "axes[0].set_title('SegNet hyperparameter search, 100 un-augmented: Train loss ')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "\n",
        "plt.legend(fontsize=6)\n",
        "\n",
        "\n",
        "iterations = len(learning_rates) * len(weight_decays)\n",
        "for i in range(iterations):\n",
        "    axes[1].plot(val_losses[i], label=combinations[i])\n",
        "\n",
        "axes[1].set_title('SegNet hyperparameter search, 100 un-augmented: Val loss')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "\n",
        "plt.legend(fontsize=6)"
      ],
      "metadata": {
        "id": "nCtRB1t9h9G_"
      },
      "id": "nCtRB1t9h9G_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "iterations = len(learning_rates) * len(weight_decays)\n",
        "\n",
        "train_f1 = [[f1.cpu().detach().numpy() for f1 in param] for param in train_f1]\n",
        "val_f1 = [[f1.cpu().detach().numpy() for f1 in param] for param in val_f1]\n",
        "\n",
        "for i in range(iterations):\n",
        "    axes[0].plot(train_f1[i], label=combinations[i], color = colors[i])\n",
        "\n",
        "axes[0].set_title('Hyperparameter fitting: Train f1')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('F1 score')\n",
        "\n",
        "plt.legend(fontsize=6)\n",
        "\n",
        "\n",
        "iterations = len(learning_rates) * len(weight_decays)\n",
        "\n",
        "for i in range(iterations):\n",
        "    axes[1].plot(val_f1[i], label=combinations[i], color = colors[i])\n",
        "\n",
        "axes[1].set_title('Hyperparameter fitting: Val f1')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('F1 score')\n",
        "\n",
        "plt.legend(fontsize=6)"
      ],
      "metadata": {
        "id": "6z0qIpMGjUz7"
      },
      "id": "6z0qIpMGjUz7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E4nSUyKfmXbI"
      },
      "id": "E4nSUyKfmXbI",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}